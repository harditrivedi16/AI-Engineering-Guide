{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Introduction to Vector Databases and Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "In this notebook, we explore how to use **vector databases** and **RAG pipelines** to retrieve and generate information from both unstructured (PDFs) and structured (Excel) data sources using Langchain."
      ],
      "metadata": {
        "id": "DvDMwEQ6sW1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a Vector Database?\n",
        "\n",
        "A vector database is a special type of database designed to store and search **embeddings** — numerical representations of text, images, or other data. These embeddings allow us to compare content based on **semantic similarity**, not just keywords.\n",
        "\n",
        "**Key features:**\n",
        "- Stores high-dimensional vectors (embeddings)\n",
        "- Enables similarity search (e.g., cosine, dot product)\n",
        "- Useful for searching across documents, FAQs, transcripts, etc.\n",
        "\n",
        "**Popular libraries/tools:**\n",
        "- FAISS (Facebook AI Similarity Search)\n",
        "- Chroma (lightweight & Langchain-native)\n",
        "- Pinecone, Weaviate, Qdrant (production-ready managed services)"
      ],
      "metadata": {
        "id": "fmpLPXEwsakZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Retrieval-Augmented Generation (RAG)?\n",
        "\n",
        "RAG is an approach that combines **retrieval systems** (like vector DBs) with **language models** to produce more accurate and context-aware responses.\n",
        "\n",
        "Instead of relying only on the model's internal knowledge, it:\n",
        "1. Retrieves relevant documents based on the query\n",
        "2. Feeds those docs into the LLM to answer the question\n",
        "\n",
        "**Why use RAG?**\n",
        "- Reduces hallucination\n",
        "- Keeps answers grounded in real, updated data\n",
        "- Supports domain-specific or private data without retraining the LLM"
      ],
      "metadata": {
        "id": "iizfTelusgr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why is This Important?\n",
        "\n",
        "- Large Language Models (LLMs) have limited memory and context windows\n",
        "- RAG pipelines allow retrieval from large corpora of custom data\n",
        "- Enables applications like:\n",
        "  - Smart document search\n",
        "  - Personalized assistants\n",
        "  - Internal knowledge bots\n",
        "  - Legal/medical document summarizers"
      ],
      "metadata": {
        "id": "3Qcfpx0Nsm9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we'll build and run end-to-end RAG pipelines on:\n",
        "- A single PDF\n",
        "- Multiple PDFs\n",
        "- Structured Excel sheets\n",
        "\n",
        "Using **Langchain**, **Chroma**, and **OpenAI embeddings**.\n"
      ],
      "metadata": {
        "id": "DC6C3Zw9sSgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Information Retrieval from a Single PDF\n",
        "\n",
        "We begin by installing and importing all necessary libraries. This includes:\n",
        "\n",
        "- `langchain`: the framework we'll use to create RAG pipelines\n",
        "- `chromadb`: lightweight in-memory vector database\n",
        "- `openai`: for generating text embeddings (you can substitute with other models too)\n",
        "- `PyPDFLoader`: to load and split PDF files into text\n",
        "- `tiktoken`: tokenizer for estimating token counts (optional but helpful)\n",
        "\n",
        "We'll also set up API keys and suppress unnecessary warnings.\n"
      ],
      "metadata": {
        "id": "ky2izFCYtO6_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgtRU9q6rPF8",
        "outputId": "1317c810-55bb-45ef-8d2b-a49da9eb1df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m110.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install core packages\n",
        "!pip install langchain langchain-community openai chromadb tiktoken pypdf --quiet\n",
        "\n",
        "# Optional: suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Splitting the PDF\n",
        "\n",
        "To work with PDFs in Langchain, we use the `PyPDFLoader` from `langchain.document_loaders`.\n",
        "\n",
        "**Why do we need to split the document?**\n",
        "- LLMs have a token limit, and feeding large PDFs directly will exceed it\n",
        "- Splitting into smaller chunks lets us embed and store them independently\n",
        "- This is the foundation for retrieval: search is performed at the chunk level\n",
        "\n",
        "We’ll:\n",
        "1. Load the Harry Potter PDF using `PyPDFLoader`\n",
        "2. Split the text into overlapping chunks using `RecursiveCharacterTextSplitter`\n",
        "   - Each chunk will have a fixed size (e.g., 500 characters) with some overlap\n",
        "   - Overlap ensures context isn't lost between chunks\n"
      ],
      "metadata": {
        "id": "OyKqzTmjuvci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "4pi3IwXZu9aN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF (upload your file if not already in Colab)\n",
        "pdf_path = \"/content/Sorcerer's Stone.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)"
      ],
      "metadata": {
        "id": "MxhgwpMhvJj2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pages\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "IEgCnLy3vMcx"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "docs = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"Loaded {len(docs)} document chunks\")\n",
        "print(docs[1].page_content[0:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL38_Tj_vOjH",
        "outputId": "85832a7b-834a-4304-c25e-fd16d8f3c2c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1225 document chunks\n",
            "ALSO BY J. K. ROWLING \n",
            "Harry Potter and the Sorcerer’s Stone \n",
            "Year One at Hogwarts \n",
            "Harry Potter and the Chamber of Secrets \n",
            "Year Two at Hogwarts \n",
            "Harry Potter and the Prisoner of Azkaban \n",
            "Year Three at Hogwarts \n",
            "Harry Potter and the Goblet of Fire \n",
            "Year Four at Hogwarts \n",
            "Harry Potter and the Order \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding and Vector Store Creation\n",
        "\n",
        "To search semantically through the document chunks, we convert them into **vector embeddings** using a pre-trained model.\n",
        "\n",
        "We’ll use:\n",
        "- `OpenAIEmbeddings` from Langchain (requires an OpenAI API key)\n",
        "- `Chroma` as the vector database to store and retrieve embeddings\n",
        "\n",
        "**Steps:**\n",
        "1. Convert each chunk into a high-dimensional vector using `OpenAIEmbeddings`\n",
        "2. Store those vectors in Chroma, which supports fast similarity search\n",
        "3. This enables us to later retrieve the most relevant chunks for a query\n",
        "\n",
        "Make sure to set your OpenAI API key before running this cell.\n"
      ],
      "metadata": {
        "id": "_MwTI-5gzTSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "import os"
      ],
      "metadata": {
        "id": "On2O1ndMzzHr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "0o-Gcq95z4i2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding model and populate vector DB\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vector_store = Chroma.from_documents(docs, embedding=embedding_model)\n",
        "\n",
        "print(\"Vector store created and ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyrtF0kt1VP2",
        "outputId": "6f7a5632-5eb7-49b0-ec1c-22cd6e32f46c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize RetrievalQA Chain for Querying\n",
        "\n",
        "To perform question answering over our PDF embeddings, we need to:\n",
        "\n",
        "- Initialize an LLM instance (`ChatOpenAI`)\n",
        "- Create a `RetrievalQA` chain combining the vector store retriever with the LLM\n",
        "\n",
        "This chain handles retrieving the most relevant document chunks and generating answers based on them.\n"
      ],
      "metadata": {
        "id": "GmEChvL83S59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "1aYmcoHO3YF9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the language model\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "LuIYHBjY3aLl"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RetrievalQA chain using the vector store as retriever\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vector_store.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"RetrievalQA chain ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WODI4lIJ3V3R",
        "outputId": "b3e83777-e573-46b5-f69c-bdba790877bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RetrievalQA chain ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Ask Questions About the PDF (RAG in Action)\n",
        "\n",
        "Now that we have:\n",
        "- Chunks of the document stored as embeddings\n",
        "- A vector database (Chroma) ready for semantic search\n",
        "- A language model (OpenAI GPT) initialized\n",
        "\n",
        "We can use **Langchain’s RetrievalQA chain** to:\n",
        "1. Accept a natural language query\n",
        "2. Retrieve the most relevant chunks from the vector store\n",
        "3. Pass those chunks as context to the LLM\n",
        "4. Generate an informed, grounded answer\n",
        "\n",
        "This is Retrieval-Augmented Generation (RAG) in action — fetching knowledge from the actual content instead of relying on model memory alone.\n"
      ],
      "metadata": {
        "id": "T4yLBRw53CqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question to the PDF\n",
        "query = \"What is the name of the school Harry Potter goes to?\"\n",
        "result = qa_chain(query)"
      ],
      "metadata": {
        "id": "v7-UXjj03hkO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the answer\n",
        "print(\"Answer:\", result['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6wQXoZC3ma5",
        "outputId": "82fce8ee-7172-42ef-ae46-08570615c66d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The school Harry Potter attends is called Hogwarts School of Witchcraft and Wizardry.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Optional) View the first retrieved source chunk\n",
        "print(\"\\n--- Source Document Sample ---\\n\")\n",
        "print(result['source_documents'][0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fj_QS0I7tdeQ",
        "outputId": "b069d8f7-8626-4c74-9791-8a942e3c9493"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Source Document Sample ---\n",
            "\n",
            "with a great destiny proves his worth while attending Hogwarts School \n",
            "of Witchcraft and Wizardry. \n",
            "ISBN 0-590-35340-3 \n",
            "[1. Fantasy — Fiction.    2. Witches — Fiction.    3. Wizards — Fiction. \n",
            "4. Schools — Fiction.    5. England — Fiction.]    I. Title. \n",
            "PZ7.R79835Har    1998 \n",
            "[Fic] — dc21    97-39059 \n",
            " \n",
            "64 65 66 67 68 69 70 71 72    05 \n",
            "Printed in U.S.A.     10 \n",
            "First American edition, October 1998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the Mirror of Erised and what does it show Harry?\"\n",
        "result = qa_chain(query)\n",
        "\n",
        "# Show the answer\n",
        "print(\"Answer:\", result['result'])\n",
        "\n",
        "# Show a snippet from the top source document\n",
        "print(\"\\n--- Source Document Excerpt ---\\n\")\n",
        "print(result['source_documents'][0].page_content[:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KUO0UBA4_cQ",
        "outputId": "30d6d2f9-e66c-4999-daa6-931809e00138"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The Mirror of Erised is a magical mirror that shows the deepest, most desperate desires of a person's heart. When Harry looks into the mirror, he sees his family, whom he has never known, standing around him. Additionally, the mirror showed Harry's friend Ron as Head Boy, reflecting his own desires and aspirations.\n",
            "\n",
            "--- Source Document Excerpt ---\n",
            "\n",
            "THE  MIRROR  OF  ERISED \n",
            " 213  \n",
            "“Strange how nearsighted being invisible can make you,” said \n",
            "Dumbledore, and Harry wa s relieved to see that he was smiling. \n",
            "“So,” said Dumbledore, slipping off the desk to sit on the floor \n",
            "with Harry, “you, like hundreds before you, have discovered the \n",
            "delights of the Mi rror of Erised.” \n",
            "“I didn’t know it was called that, sir.” \n",
            "“But I expect you’ve realized by now what it does?” \n",
            "“It — well — it show s me my family —”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Ingesting Multiple PDFs\n",
        "\n",
        "In real-world applications, you often need to retrieve information from **multiple documents** (e.g., reports, research papers, product manuals).\n",
        "\n",
        "Langchain and Chroma make this simple:\n",
        "- You load each PDF separately\n",
        "- Split their content into chunks\n",
        "- Combine all the chunks into a single vector store\n",
        "\n",
        "This unified store lets your retrieval pipeline pull relevant context across all uploaded sources.\n",
        "\n",
        "**Use case examples:**\n",
        "- Search across policy documents\n",
        "- Ask questions across multiple case files\n",
        "- Build multi-source knowledge bots\n"
      ],
      "metadata": {
        "id": "oJRhwMH95Y6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip Folder and Prepare PDFs\n",
        "\n",
        "We’ve uploaded a zipped folder containing multiple PDFs (`Harry Potter.zip`). Now we’ll:\n",
        "\n",
        "1. Unzip the folder\n",
        "2. Collect all `.pdf` file paths\n",
        "3. Prepare them for processing\n",
        "\n",
        "This sets us up to ingest all documents in a loop and build a combined vector store.\n"
      ],
      "metadata": {
        "id": "lXxsQeTi5-wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "id": "1s2Mfr0Q6EjJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the uploaded folder\n",
        "zip_path = \"/content/Harry Potter.zip\"\n",
        "extract_dir = \"/content/harry_potter_pdfs\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "cwIaM-vT6HCg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "LjsUY94x7G5N"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the actual PDF folder\n",
        "pdf_dir = \"/content/harry_potter_pdfs/Harry Potter\"\n",
        "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
        "\n",
        "print(f\"Found {len(pdf_files)} PDF files in subfolder:\\n\")\n",
        "for f in pdf_files:\n",
        "    print(\"-\", os.path.basename(f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnF-_e_55RGM",
        "outputId": "c89f2de9-4d61-4137-983c-a5b5f9fd6690"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 PDF files in subfolder:\n",
            "\n",
            "- Chamber of Secrets.pdf\n",
            "- prizoner of Azkaban.pdf\n",
            "- Sorcerer's Stone.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Chunk All PDFs\n",
        "\n",
        "Now we’ll process all PDFs together:\n",
        "\n",
        "1. Load each PDF using `PyPDFLoader`\n",
        "2. Extract its pages as `Document` objects\n",
        "3. Split all pages into chunks using `RecursiveCharacterTextSplitter`\n",
        "\n",
        "All chunks will be stored in a single list, which we’ll later embed and store in a combined vector DB.\n",
        "\n",
        "**Why do this?**\n",
        "- It allows unified search across all documents\n",
        "- Preserves semantic relationships per chunk\n",
        "- Efficient and scalable for multi-source RAG\n"
      ],
      "metadata": {
        "id": "CRBQQeUT7S4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "eLofj8Bs7XD0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs = []\n",
        "\n",
        "# Loop through all PDFs and split\n",
        "for path in pdf_files:\n",
        "    loader = PyPDFLoader(path)\n",
        "    pages = loader.load()\n",
        "\n",
        "    # Split pages into chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = splitter.split_documents(pages)\n",
        "\n",
        "    all_docs.extend(chunks)\n",
        "\n",
        "print(f\"Total chunks across all PDFs: {len(all_docs)}\")\n",
        "print(\"Sample chunk:\\n\", all_docs[0].page_content[:400])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIG11idq7RTl",
        "outputId": "8a3ef6ca-b792-4bc7-b3e2-a6ce62159d4d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks across all PDFs: 3377\n",
            "Sample chunk:\n",
            " Harry Potter and the Chamber\n",
            "of Secrets PDF\n",
            "J.K. Rowling\n",
            "Scan to Download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed and Store All Chunks in a Unified Vector Store\n",
        "\n",
        "With all document chunks prepared, we now:\n",
        "\n",
        "1. Use `OpenAIEmbeddings` to convert each chunk to a vector\n",
        "2. Store all vectors in one `Chroma` instance\n",
        "\n",
        "This enables cross-document retrieval — meaning you can ask questions that span across multiple Harry Potter books or chapters.\n",
        "\n",
        "We'll reuse the embedding model setup, and store everything in-memory for now.\n"
      ],
      "metadata": {
        "id": "sNkvBRvB7kPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Initialize embedding model\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Create a single Chroma vector store for all chunks\n",
        "multi_doc_vector_store = Chroma.from_documents(all_docs, embedding=embedding_model)\n",
        "\n",
        "print(\"Multi-document vector store is ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_IQCCJL7hX9",
        "outputId": "c7e717e1-ddc7-4249-e507-1a228a108ad5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-document vector store is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup RetrievalQA Chain for Multi-Document Retrieval\n",
        "\n",
        "Now that all PDFs are embedded in one vector store, we set up the `RetrievalQA` chain again, but this time using `multi_doc_vector_store`.\n",
        "\n",
        "This allows us to:\n",
        "- Ask questions without knowing which PDF contains the answer\n",
        "- Retrieve relevant chunks across all documents\n",
        "- Keep the pipeline exactly the same as before — just with more data\n"
      ],
      "metadata": {
        "id": "uC59eIbr7wR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Reinitialize the LLM (if not done already)\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Create RetrievalQA with new multi-PDF retriever\n",
        "multi_doc_qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=multi_doc_vector_store.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"Multi-document QA chain initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKSzCYX_7uSR",
        "outputId": "484051fc-c373-49be-dd90-cc5da8f80e93"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-document QA chain initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ask Questions Across All PDFs\n",
        "\n",
        "Let’s test the multi-document RAG pipeline with a query that may be answered in **any one of the PDFs**.\n",
        "\n",
        ">  **Query**: \"Who gives Harry the invisibility cloak and when?\"\n",
        "\n",
        "This checks whether:\n",
        "- Relevant chunks are correctly retrieved from the right document\n",
        "- The model synthesizes context into a meaningful, accurate answer\n",
        "\n",
        "This simulates how multi-source retrieval would work in real applications like case files, legal docs, or product manuals.\n"
      ],
      "metadata": {
        "id": "BmgYh8km77fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What happens when Harry speaks Parseltongue in the dueling club?\"\n",
        "result = multi_doc_qa_chain(query)\n",
        "\n",
        "# Show the answer\n",
        "print(\"Answer:\", result['result'])\n",
        "\n",
        "# Show excerpt of retrieved source\n",
        "print(\"\\n--- Source Document Snippet ---\\n\")\n",
        "print(result['source_documents'][0].page_content[:700])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiWKZmlL77GT",
        "outputId": "0277301b-f3f8-4211-f28f-4b7bcb25f4dc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: When Harry speaks Parseltongue in the dueling club, he unintentionally reveals his ability to communicate with snakes. This causes confusion and fear among the other students, as speaking Parseltongue is traditionally associated with dark wizards. Harry's actions are misinterpreted, leading to a sense of isolation and concern about how others perceive him.\n",
            "\n",
            "--- Source Document Snippet ---\n",
            "\n",
            "tries to protect Harry. Each character faces their fears and\n",
            "challenges, emphasizing that courage can manifest in many\n",
            "forms.\n",
            "Chapter 11 | THE DUELING CLUB| Q&A\n",
            "1.Question\n",
            "What does Harry's experience as a Parselmouth reveal\n",
            "about his character, and how does it affect his\n",
            "relationships with others?\n",
            "Answer:Harry discovers that he can speak\n",
            "Parseltongue, the language of snakes, which is\n",
            "traditionally associated with dark wizards,\n",
            "Scan to Download\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Working with Structured Data (Excel Sheets)\n",
        "\n",
        "Besides unstructured PDFs, many real-world datasets exist in structured formats like Excel spreadsheets.\n",
        "\n",
        "We’ll:\n",
        "- Create a sample Excel sheet representing tabular data (e.g., student records or book info)\n",
        "- Load it using `pandas`\n",
        "- Convert rows into Langchain `Document` objects\n",
        "- Embed and store them in a vector database\n",
        "- Query the data with the same RAG approach\n",
        "\n",
        "This extends our RAG pipeline beyond text documents to structured datasets.\n"
      ],
      "metadata": {
        "id": "QRULfAh-8Sma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data: Harry Potter characters info\n",
        "data = {\n",
        "    \"Name\": [\"Harry Potter\", \"Hermione Granger\", \"Ron Weasley\", \"Albus Dumbledore\"],\n",
        "    \"House\": [\"Gryffindor\", \"Gryffindor\", \"Gryffindor\", \"Gryffindor\"],\n",
        "    \"Role\": [\"Student\", \"Student\", \"Student\", \"Headmaster\"],\n",
        "    \"Wand\": [\n",
        "        \"11-inch Holly, Phoenix feather\",\n",
        "        \"10¾-inch Vine wood, Dragon heartstring\",\n",
        "        \"14-inch Willow, Unicorn hair\",\n",
        "        \"15-inch Elder, Thestral tail hair\"\n",
        "    ],\n",
        "    \"Patronus\": [\"Stag\", \"Otter\", \"Jack Russell Terrier\", \"Phoenix\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save as Excel\n",
        "excel_path = \"/content/harry_potter_characters.xlsx\"\n",
        "df.to_excel(excel_path, index=False)\n",
        "\n",
        "print(f\"Sample Excel sheet saved at {excel_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHS2-PrL8UGj",
        "outputId": "56b2b049-f0bb-4d5f-bc8f-596b533ae45e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Excel sheet saved at /content/harry_potter_characters.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Excel and Convert Rows to Documents\n",
        "\n",
        "We’ll load the Excel sheet and treat each row as a separate “document” containing concatenated cell values.\n",
        "\n",
        "This allows us to:\n",
        "- Create embeddings for each row\n",
        "- Search across structured data with semantic queries\n",
        "- Use the same RAG approach as with PDFs\n",
        "\n",
        "Each row’s content is combined into a single text string to be embedded.\n"
      ],
      "metadata": {
        "id": "1O8msFwu8l2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "df = pd.read_excel(excel_path)\n",
        "\n",
        "# Convert each row to a Document\n",
        "docs = []\n",
        "for _, row in df.iterrows():\n",
        "    content = \", \".join([f\"{col}: {val}\" for col, val in row.items()])\n",
        "    docs.append(Document(page_content=content))\n",
        "\n",
        "print(f\" Converted {len(docs)} rows to Documents. Sample:\\n\")\n",
        "print(docs[0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sGaldKr8kHB",
        "outputId": "c52a0f04-efce-424f-e8a5-4a9120c32459"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Converted 4 rows to Documents. Sample:\n",
            "\n",
            "Name: Harry Potter, House: Gryffindor, Role: Student, Wand: 11-inch Holly, Phoenix feather, Patronus: Stag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embed and Store Excel Rows in Vector Store\n",
        "\n",
        "We will use the same embedding model (`OpenAIEmbeddings`) to convert each row into a vector and store them in Chroma.\n",
        "\n",
        "This lets us run semantic searches over structured tabular data just like unstructured text.\n"
      ],
      "metadata": {
        "id": "zhJikhcC8vKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Initialize embedding model (reuse if already initialized)\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Create vector store from the Excel row documents\n",
        "excel_vector_store = Chroma.from_documents(docs, embedding=embedding_model)\n",
        "\n",
        "print(\" Excel data embedded and stored in vector store.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-KmT1Sm8rwU",
        "outputId": "09dd4d4e-a4d7-4c0a-e056-829ff697a6e1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Excel data embedded and stored in vector store.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup RetrievalQA Chain for Excel Data\n",
        "\n",
        "We’ll use the same approach as before:\n",
        "\n",
        "- Initialize the LLM (`ChatOpenAI`)\n",
        "- Use the Excel vector store’s retriever\n",
        "- Create a RetrievalQA chain to answer queries over the structured data\n"
      ],
      "metadata": {
        "id": "jNYTTSLz826Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Initialize LLM (reuse if possible)\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Create RetrievalQA with Excel vector store retriever\n",
        "excel_qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=excel_vector_store.as_retriever(),\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\" RetrievalQA chain for Excel data ready.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0svDylzQ8yim",
        "outputId": "f16004c4-836e-4226-eccb-149815ae78ab"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " RetrievalQA chain for Excel data ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the patronus of Hermione Granger?\"\n",
        "result = excel_qa_chain(query)\n",
        "\n",
        "print(\"Answer:\", result['result'])\n",
        "\n",
        "print(\"\\n--- Source Document ---\\n\")\n",
        "print(result['source_documents'][0].page_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwTjJ17186mi",
        "outputId": "48289c9c-9e4e-44d1-fd9f-4f13e9d5cfe1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Hermione Granger's Patronus is an otter.\n",
            "\n",
            "--- Source Document ---\n",
            "\n",
            "Name: Hermione Granger, House: Gryffindor, Role: Student, Wand: 10¾-inch Vine wood, Dragon heartstring, Patronus: Otter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKKCoVNv9LPc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}